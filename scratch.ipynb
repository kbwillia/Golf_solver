{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "class Card:\n",
    "    def __init__(self, rank, suit):\n",
    "        self.rank = rank\n",
    "        self.suit = suit\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.rank}{self.suit}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "    def score(self):\n",
    "        if self.rank == 'A':\n",
    "            return 1\n",
    "        elif self.rank == 'J':\n",
    "            return 0\n",
    "        elif self.rank in ['Q', 'K']:\n",
    "            return 10\n",
    "        else:\n",
    "            return int(self.rank)\n",
    "\n",
    "class GolfGame:\n",
    "    RANKS = ['A', '2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K']\n",
    "    SUITS = ['♠', '♥', '♦', '♣']\n",
    "\n",
    "    def __init__(self):\n",
    "        self.deck = self.create_deck()\n",
    "        self.grid = [None, None, None, None]  # top-left, top-right, bottom-left, bottom-right\n",
    "        self.known_cards = [False, False, True, True]  # initially know bottom 2 cards\n",
    "        self.turn = 1\n",
    "        self.discard_pile = []  # Complete history of discarded cards\n",
    "        self.opponents_visible = []  # Cards visible in opponents' grids\n",
    "        self.memory = {\n",
    "            'all_seen_cards': [],  # Every card we've seen\n",
    "            'discard_history': [],  # Full discard pile\n",
    "            'cards_per_rank': {rank: 0 for rank in self.RANKS}  # Count of seen cards per rank\n",
    "        }\n",
    "\n",
    "    def create_deck(self):\n",
    "        return [Card(rank, suit) for rank, suit in itertools.product(self.RANKS, self.SUITS)]\n",
    "\n",
    "    def calculate_score(self, grid):\n",
    "        \"\"\"Calculate score for a 2x2 grid with pair cancellation\"\"\"\n",
    "        scores = [card.score() if card else 0 for card in grid]\n",
    "        total_score = sum(scores)\n",
    "\n",
    "        # Check for pairs that cancel (any two matching ranks)\n",
    "        ranks = [card.rank if card else None for card in grid]\n",
    "        pairs = []\n",
    "        used_positions = set()\n",
    "\n",
    "        # Check all possible pairs (6 combinations for 4 positions)\n",
    "        for pos1, pos2 in itertools.combinations(range(4), 2):\n",
    "            if (ranks[pos1] and ranks[pos2] and\n",
    "                ranks[pos1] == ranks[pos2] and\n",
    "                pos1 not in used_positions and pos2 not in used_positions):\n",
    "                pairs.append((pos1, pos2))\n",
    "                used_positions.add(pos1)\n",
    "                used_positions.add(pos2)\n",
    "                total_score -= (scores[pos1] + scores[pos2])\n",
    "\n",
    "        return total_score, pairs\n",
    "\n",
    "    def update_memory(self, new_cards):\n",
    "        \"\"\"Update memory with newly seen cards\"\"\"\n",
    "        for card in new_cards:\n",
    "            if card and card not in self.memory['all_seen_cards']:\n",
    "                self.memory['all_seen_cards'].append(card)\n",
    "                self.memory['cards_per_rank'][card.rank] += 1\n",
    "\n",
    "    def add_to_discard(self, card):\n",
    "        \"\"\"Add card to discard pile and update memory\"\"\"\n",
    "        if card:\n",
    "            self.memory['discard_history'].append(card)\n",
    "            self.update_memory([card])\n",
    "\n",
    "    def get_deck_probabilities(self, additional_seen_cards=None):\n",
    "        \"\"\"Calculate probability distribution of remaining cards in deck using full memory\"\"\"\n",
    "        # Start with cards we've tracked in memory\n",
    "        rank_counts = self.memory['cards_per_rank'].copy()\n",
    "\n",
    "        # Add any additional cards passed in (for temporary calculations)\n",
    "        if additional_seen_cards:\n",
    "            for card in additional_seen_cards:\n",
    "                if card:\n",
    "                    rank_counts[card.rank] += 1\n",
    "\n",
    "        # Calculate remaining cards for each rank (4 total per rank in deck)\n",
    "        remaining_cards = {}\n",
    "        for rank in self.RANKS:\n",
    "            remaining_cards[rank] = max(0, 4 - rank_counts[rank])\n",
    "\n",
    "        total_remaining = sum(remaining_cards.values())\n",
    "\n",
    "        # Convert to probabilities\n",
    "        probabilities = {}\n",
    "        for rank in self.RANKS:\n",
    "            probabilities[rank] = remaining_cards[rank] / total_remaining if total_remaining > 0 else 0\n",
    "\n",
    "        return probabilities, total_remaining, rank_counts\n",
    "\n",
    "    def expected_score_for_unknown_position(self, probabilities):\n",
    "        \"\"\"Calculate expected score for an unknown card position\"\"\"\n",
    "        expected = 0\n",
    "        for rank, prob in probabilities.items():\n",
    "            card_score = Card(rank, '♠').score()  # suit doesn't matter for scoring\n",
    "            expected += prob * card_score\n",
    "        return expected\n",
    "\n",
    "    def get_memory_analysis(self):\n",
    "        \"\"\"Get detailed analysis of what we've seen\"\"\"\n",
    "        probs, total_remaining, seen_counts = self.get_deck_probabilities()\n",
    "\n",
    "        analysis = {\n",
    "            'total_cards_seen': len(self.memory['all_seen_cards']),\n",
    "            'total_remaining_in_deck': total_remaining,\n",
    "            'discard_pile_size': len(self.memory['discard_history']),\n",
    "            'rank_analysis': {}\n",
    "        }\n",
    "\n",
    "        for rank in self.RANKS:\n",
    "            remaining = 4 - seen_counts[rank]\n",
    "            analysis['rank_analysis'][rank] = {\n",
    "                'seen': seen_counts[rank],\n",
    "                'remaining': remaining,\n",
    "                'probability': probs[rank],\n",
    "                'score': Card(rank, '♠').score()\n",
    "            }\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    def evaluate_take_discard_action(self, position, discard_card, current_grid, known_cards):\n",
    "        \"\"\"Evaluate taking discard card and placing it at position\"\"\"\n",
    "        new_grid = current_grid.copy()\n",
    "        new_grid[position] = discard_card\n",
    "        new_known = known_cards.copy()\n",
    "        new_known[position] = True\n",
    "\n",
    "        # Calculate score with this new card\n",
    "        known_score, pairs = self.calculate_score([card if new_known[i] else None for i, card in enumerate(new_grid)])\n",
    "\n",
    "        # Add expected score for unknown positions using memory\n",
    "        deck_probs, _, _ = self.get_deck_probabilities([discard_card])  # Include the card we're taking\n",
    "        unknown_expected = 0\n",
    "        for i in range(4):\n",
    "            if not new_known[i]:\n",
    "                unknown_expected += self.expected_score_for_unknown_position(deck_probs)\n",
    "\n",
    "        total_expected = known_score + unknown_expected\n",
    "\n",
    "        return {\n",
    "            'action': f'Take {discard_card} → Position {position + 1}',\n",
    "            'expected_score': total_expected,\n",
    "            'known_score': known_score,\n",
    "            'pairs': pairs,\n",
    "            'position': position\n",
    "        }\n",
    "\n",
    "    def evaluate_draw_deck_action(self, position, current_grid, known_cards):\n",
    "        \"\"\"Evaluate drawing from deck and expected outcome at position\"\"\"\n",
    "        deck_probs, total_remaining, _ = self.get_deck_probabilities()\n",
    "\n",
    "        if total_remaining == 0:\n",
    "            return {\n",
    "                'action': f'Draw from deck → Position {position + 1}',\n",
    "                'expected_score': float('inf'),  # No cards left\n",
    "                'position': position,\n",
    "                'error': 'No cards remaining in deck'\n",
    "            }\n",
    "\n",
    "        total_expected_score = 0\n",
    "        best_cards = []\n",
    "\n",
    "        for rank, prob in deck_probs.items():\n",
    "            if prob == 0:\n",
    "                continue\n",
    "\n",
    "            # Simulate drawing this card\n",
    "            drawn_card = Card(rank, '♠')  # suit doesn't matter\n",
    "            new_grid = current_grid.copy()\n",
    "            new_grid[position] = drawn_card\n",
    "            new_known = known_cards.copy()\n",
    "            new_known[position] = True\n",
    "\n",
    "            # Calculate score\n",
    "            known_score, pairs = self.calculate_score([card if new_known[i] else None for i, card in enumerate(new_grid)])\n",
    "\n",
    "            # Add expected score for remaining unknown positions\n",
    "            remaining_deck_probs, _, _ = self.get_deck_probabilities([drawn_card])\n",
    "            unknown_expected = 0\n",
    "            for i in range(4):\n",
    "                if not new_known[i]:\n",
    "                    unknown_expected += self.expected_score_for_unknown_position(remaining_deck_probs)\n",
    "\n",
    "            total_score = known_score + unknown_expected\n",
    "            total_expected_score += prob * total_score\n",
    "\n",
    "            if drawn_card.score() <= 1:  # Good cards (A=1, J=0)\n",
    "                best_cards.append((rank, prob))\n",
    "\n",
    "        return {\n",
    "            'action': f'Draw from deck → Position {position + 1}',\n",
    "            'expected_score': total_expected_score,\n",
    "            'position': position,\n",
    "            'prob_good_card': sum(prob for rank, prob in best_cards),\n",
    "            'best_possible': best_cards,\n",
    "            'cards_remaining': total_remaining\n",
    "        }\n",
    "\n",
    "    def get_recommendations(self, grid, known_cards, discard_top, turn):\n",
    "        \"\"\"Get action recommendations with probabilities using full memory\"\"\"\n",
    "        # Update memory with current known cards and discard top\n",
    "        current_known = [card for i, card in enumerate(grid) if known_cards[i] and card]\n",
    "        self.update_memory(current_known + [discard_top])\n",
    "\n",
    "        current_score, current_pairs = self.calculate_score([card if known_cards[i] else None for i, card in enumerate(grid)])\n",
    "\n",
    "        # Calculate baseline expected score (doing nothing)\n",
    "        deck_probs, total_remaining, _ = self.get_deck_probabilities()\n",
    "        baseline_unknown_expected = 0\n",
    "        for i in range(4):\n",
    "            if not known_cards[i]:\n",
    "                baseline_unknown_expected += self.expected_score_for_unknown_position(deck_probs)\n",
    "        baseline_expected = current_score + baseline_unknown_expected\n",
    "\n",
    "        recommendations = []\n",
    "\n",
    "        # Available positions (face-down cards only)\n",
    "        available_positions = [i for i in range(4) if not known_cards[i]]\n",
    "\n",
    "        if not available_positions:\n",
    "            return {\n",
    "                \"message\": \"No moves available - all cards are face-up!\",\n",
    "                \"baseline_score\": current_score,\n",
    "                \"memory_analysis\": self.get_memory_analysis()\n",
    "            }\n",
    "\n",
    "        # Evaluate taking discard card\n",
    "        for pos in available_positions:\n",
    "            eval_result = self.evaluate_take_discard_action(pos, discard_top, grid, known_cards)\n",
    "            improvement = baseline_expected - eval_result['expected_score']\n",
    "            confidence = min(95, max(5, 50 + improvement * 15))\n",
    "\n",
    "            # Check if this creates a pair\n",
    "            creates_pair = any(card and card.rank == discard_top.rank for i, card in enumerate(grid) if known_cards[i])\n",
    "\n",
    "            recommendations.append({\n",
    "                **eval_result,\n",
    "                'improvement': improvement,\n",
    "                'confidence': confidence,\n",
    "                'type': 'take_discard',\n",
    "                'creates_pair': creates_pair\n",
    "            })\n",
    "\n",
    "        # Evaluate drawing from deck\n",
    "        for pos in available_positions:\n",
    "            eval_result = self.evaluate_draw_deck_action(pos, grid, known_cards)\n",
    "            if 'error' in eval_result:\n",
    "                continue\n",
    "\n",
    "            improvement = baseline_expected - eval_result['expected_score']\n",
    "            confidence = min(85, max(10, 40 + improvement * 10))  # Lower confidence for unknown cards\n",
    "\n",
    "            recommendations.append({\n",
    "                **eval_result,\n",
    "                'improvement': improvement,\n",
    "                'confidence': confidence,\n",
    "                'type': 'draw_deck'\n",
    "            })\n",
    "\n",
    "        # Sort by improvement (best first)\n",
    "        recommendations.sort(key=lambda x: x['improvement'], reverse=True)\n",
    "\n",
    "        return {\n",
    "            'recommendations': recommendations,\n",
    "            'baseline_score': baseline_expected,\n",
    "            'current_known_score': current_score,\n",
    "            'current_pairs': current_pairs,\n",
    "            'turn': turn,\n",
    "            'memory_analysis': self.get_memory_analysis()\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    \"\"\"Example usage of the Golf solver with memory tracking\"\"\"\n",
    "    game = GolfGame()\n",
    "\n",
    "    # Simulate a game in progress - add some cards to memory\n",
    "    # Cards that have been discarded throughout the game\n",
    "    previous_discards = [\n",
    "        Card('K', '♠'), Card('9', '♥'), Card('Q', '♦'), Card('8', '♣'), Card('A', '♠')\n",
    "    ]\n",
    "\n",
    "    for card in previous_discards:\n",
    "        game.add_to_discard(card)\n",
    "\n",
    "    # Example current game state\n",
    "    grid = [\n",
    "        None,  # top-left (unknown)\n",
    "        Card('Q', '♠'),  # top-right (known, face-up)\n",
    "        Card('6', '♥'),  # bottom-left (known from start)\n",
    "        Card('J', '♣')   # bottom-right (known from start)\n",
    "    ]\n",
    "\n",
    "    known_cards = [False, True, True, True]\n",
    "    discard_top = Card('6', '♠')  # Current top of discard pile\n",
    "    turn = 2\n",
    "\n",
    "    print(\"=== 4-CARD GOLF STRATEGY SOLVER (with Memory) ===\\n\")\n",
    "    print(\"Current Grid:\")\n",
    "    print(f\"[ {'?' if not known_cards[0] else str(grid[0])} | {grid[1] if known_cards[1] else '?'} ]\")\n",
    "    print(f\"[ {grid[2] if known_cards[2] else '?'} | {grid[3] if known_cards[3] else '?'} ]\")\n",
    "    print(f\"\\nDiscard pile top: {discard_top}\")\n",
    "    print(f\"Turn: {turn}/4\\n\")\n",
    "\n",
    "    # Get recommendations\n",
    "    result = game.get_recommendations(grid, known_cards, discard_top, turn)\n",
    "\n",
    "    if 'message' in result:\n",
    "        print(result['message'])\n",
    "        print(f\"Final score: {result['baseline_score']}\")\n",
    "        return\n",
    "\n",
    "    # Show memory analysis\n",
    "    memory = result['memory_analysis']\n",
    "    print(\"=== MEMORY ANALYSIS ===\")\n",
    "    print(f\"Cards seen: {memory['total_cards_seen']}\")\n",
    "    print(f\"Cards remaining in deck: {memory['total_remaining_in_deck']}\")\n",
    "    print(f\"Discard pile size: {memory['discard_pile_size']}\")\n",
    "\n",
    "    print(\"\\nRank probabilities remaining:\")\n",
    "    for rank in ['A', 'J', '2', '6', 'Q', 'K']:  # Show key ranks\n",
    "        info = memory['rank_analysis'][rank]\n",
    "        print(f\"  {rank}: {info['remaining']}/4 left ({info['probability']:.1%}) - Score: {info['score']}\")\n",
    "\n",
    "    print(f\"\\nCurrent known score: {result['current_known_score']}\")\n",
    "    if result['current_pairs']:\n",
    "        pairs_str = ', '.join([f\"Pos {p1+1} & {p2+1}\" for p1, p2 in result['current_pairs']])\n",
    "        print(f\"Current pairs: {pairs_str}\")\n",
    "    print(f\"Expected final score if no action: {result['baseline_score']:.1f}\\n\")\n",
    "\n",
    "    print(\"RECOMMENDATIONS (best first):\\n\")\n",
    "\n",
    "    for i, rec in enumerate(result['recommendations'][:5]):  # Show top 5\n",
    "        rank = \"🥇\" if i == 0 else \"🥈\" if i == 1 else \"🥉\" if i == 2 else f\"{i+1}.\"\n",
    "\n",
    "        print(f\"{rank} {rec['action']}\")\n",
    "        print(f\"   Expected improvement: {rec['improvement']:+.1f} points\")\n",
    "        print(f\"   Confidence: {rec['confidence']:.0f}%\")\n",
    "        print(f\"   Expected final score: {rec['expected_score']:.1f}\")\n",
    "\n",
    "        if rec['type'] == 'take_discard' and rec.get('creates_pair'):\n",
    "            print(\"   ⭐ CREATES A PAIR! ⭐\")\n",
    "\n",
    "        if rec['type'] == 'draw_deck':\n",
    "            print(f\"   Chance of good card (A, J): {rec['prob_good_card']:.1%}\")\n",
    "            if 'cards_remaining' in rec:\n",
    "                print(f\"   Cards left in deck: {rec['cards_remaining']}\")\n",
    "\n",
    "        print()\n",
    "\n",
    "    # Strategy advice based on memory\n",
    "    print(\"STRATEGY NOTES:\")\n",
    "    best_action = result['recommendations'][0]\n",
    "\n",
    "    if best_action['improvement'] > 2:\n",
    "        print(\"• Strong move available - high confidence recommendation\")\n",
    "    elif best_action['improvement'] > 0.5:\n",
    "        print(\"• Decent improvement possible\")\n",
    "    elif best_action['improvement'] > -0.5:\n",
    "        print(\"• Marginal decision - consider position and remaining turns\")\n",
    "    else:\n",
    "        print(\"• No great options - might want to play conservatively\")\n",
    "\n",
    "    if turn >= 3:\n",
    "        print(\"• Late in the game - prioritize certainty over potential\")\n",
    "\n",
    "    # Memory-based insights\n",
    "    good_cards_left = sum(memory['rank_analysis'][rank]['remaining'] for rank in ['A', 'J'])\n",
    "    total_left = memory['total_remaining_in_deck']\n",
    "    if total_left > 0:\n",
    "        good_card_prob = good_cards_left / total_left\n",
    "        print(f\"• {good_cards_left} good cards (A, J) left in {total_left} remaining cards ({good_card_prob:.1%})\")\n",
    "\n",
    "    # Check for pair opportunities\n",
    "    if any(rec.get('creates_pair') for rec in result['recommendations'] if rec['type'] == 'take_discard'):\n",
    "        print(\"• 🎯 PAIR OPPORTUNITY: Taking discard creates a matching pair!\")\n",
    "\n",
    "    # Check if specific ranks are depleted\n",
    "    depleted_ranks = [rank for rank, info in memory['rank_analysis'].items() if info['remaining'] == 0]\n",
    "    if depleted_ranks:\n",
    "        print(f\"• No more {', '.join(depleted_ranks)} cards available\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict, deque\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "import json\n",
    "\n",
    "class Card:\n",
    "    def __init__(self, rank: str, suit: str):\n",
    "        self.rank = rank\n",
    "        self.suit = suit\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.rank}{self.suit}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "    def score(self):\n",
    "        if self.rank == 'A':\n",
    "            return 1\n",
    "        elif self.rank == 'J':\n",
    "            return 0\n",
    "        elif self.rank in ['Q', 'K']:\n",
    "            return 10\n",
    "        else:\n",
    "            return int(self.rank)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, Card):\n",
    "            return False\n",
    "        return self.rank == other.rank and self.suit == other.suit\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.rank, self.suit))\n",
    "\n",
    "class Player:\n",
    "    def __init__(self, name, agent_type=\"random\"):\n",
    "        self.name = name\n",
    "        self.agent_type = agent_type\n",
    "        self.grid = [None] * 4  # 2x2 grid: [TL, TR, BL, BR]\n",
    "        self.known = [False, False, True, True]  # Only bottom two known at start\n",
    "        # Memory for tracking seen cards\n",
    "        self.memory = {\n",
    "            'all_seen_cards': [],\n",
    "            'discard_history': [],\n",
    "            'cards_per_rank': {rank: 0 for rank in ['A', '2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K']}\n",
    "        }\n",
    "\n",
    "    def reveal_all(self):\n",
    "        self.known = [True] * 4\n",
    "\n",
    "    def __str__(self):\n",
    "        def show(i):\n",
    "            return str(self.grid[i]) if self.known[i] else '?'\n",
    "        return f\"[ {show(0)} | {show(1)} ]\\n[ {show(2)} | {show(3)} ]\"\n",
    "\n",
    "    def update_memory(self, new_cards):\n",
    "        \"\"\"Update memory with newly seen cards\"\"\"\n",
    "        for card in new_cards:\n",
    "            if card and card not in self.memory['all_seen_cards']:\n",
    "                self.memory['all_seen_cards'].append(card)\n",
    "                self.memory['cards_per_rank'][card.rank] += 1\n",
    "\n",
    "    def add_to_discard_memory(self, card):\n",
    "        \"\"\"Add card to discard pile memory\"\"\"\n",
    "        if card:\n",
    "            self.memory['discard_history'].append(card)\n",
    "            self.update_memory([card])\n",
    "\n",
    "    def get_deck_probabilities(self, additional_seen_cards=None):\n",
    "        \"\"\"Calculate probability distribution of remaining cards in deck\"\"\"\n",
    "        rank_counts = self.memory['cards_per_rank'].copy()\n",
    "\n",
    "        if additional_seen_cards:\n",
    "            for card in additional_seen_cards:\n",
    "                if card:\n",
    "                    rank_counts[card.rank] += 1\n",
    "\n",
    "        remaining_cards = {}\n",
    "        for rank in ['A', '2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K']:\n",
    "            remaining_cards[rank] = max(0, 4 - rank_counts[rank])\n",
    "\n",
    "        total_remaining = sum(remaining_cards.values())\n",
    "        probabilities = {}\n",
    "        for rank in ['A', '2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K']:\n",
    "            probabilities[rank] = remaining_cards[rank] / total_remaining if total_remaining > 0 else 0\n",
    "\n",
    "        return probabilities, total_remaining\n",
    "\n",
    "    def expected_score_for_unknown_position(self, probabilities):\n",
    "        \"\"\"Calculate expected score for an unknown card position\"\"\"\n",
    "        expected = 0\n",
    "        for rank, prob in probabilities.items():\n",
    "            card_score = Card(rank, '♠').score()\n",
    "            expected += prob * card_score\n",
    "        return expected\n",
    "\n",
    "class RandomAgent:\n",
    "    \"\"\"Random agent that makes random legal moves\"\"\"\n",
    "    def choose_action(self, player, game_state, trajectory=None):\n",
    "        positions = [i for i, known in enumerate(player.known) if not known]\n",
    "        if not positions:\n",
    "            return None\n",
    "\n",
    "        action = random.choice(['draw_deck', 'take_discard'])\n",
    "        pos = random.choice(positions)\n",
    "\n",
    "        if action == 'take_discard' and game_state.discard_pile:\n",
    "            return {'type': 'take_discard', 'position': pos}\n",
    "        else:\n",
    "            # For draw_deck, also decide whether to keep the card\n",
    "            keep = random.choice([True, False])\n",
    "            return {'type': 'draw_deck', 'position': pos, 'keep': keep}\n",
    "\n",
    "class HeuristicAgent:\n",
    "    \"\"\"Heuristic agent using strategy from original main.py\"\"\"\n",
    "    def choose_action(self, player, game_state, trajectory=None):\n",
    "        positions = [i for i, known in enumerate(player.known) if not known]\n",
    "        if not positions:\n",
    "            return None\n",
    "\n",
    "        # Update memory with current known cards and discard top\n",
    "        current_known = [card for i, card in enumerate(player.grid) if player.known[i] and card]\n",
    "        discard_top = game_state.discard_pile[-1] if game_state.discard_pile else None\n",
    "        player.update_memory(current_known + ([discard_top] if discard_top else []))\n",
    "\n",
    "        # Calculate current score\n",
    "        current_score = self.calculate_score([card if player.known[i] else None for i, card in enumerate(player.grid)])\n",
    "\n",
    "        # Get deck probabilities\n",
    "        deck_probs, total_remaining = player.get_deck_probabilities()\n",
    "\n",
    "        # Calculate baseline expected score (doing nothing)\n",
    "        baseline_unknown_expected = 0\n",
    "        for i in range(4):\n",
    "            if not player.known[i]:\n",
    "                baseline_unknown_expected += player.expected_score_for_unknown_position(deck_probs)\n",
    "        baseline_expected = current_score + baseline_unknown_expected\n",
    "\n",
    "        best_action = None\n",
    "        best_improvement = float('-inf')\n",
    "\n",
    "        # Evaluate taking discard card\n",
    "        if discard_top:\n",
    "            for pos in positions:\n",
    "                improvement = self.evaluate_take_discard_action(pos, discard_top, player, deck_probs, baseline_expected)\n",
    "                if improvement > best_improvement:\n",
    "                    best_improvement = improvement\n",
    "                    best_action = {'type': 'take_discard', 'position': pos}\n",
    "\n",
    "        # Evaluate drawing from deck\n",
    "        for pos in positions:\n",
    "            improvement = self.evaluate_draw_deck_action(pos, player, deck_probs, baseline_expected)\n",
    "            if improvement > best_improvement:\n",
    "                best_improvement = improvement\n",
    "                best_action = {'type': 'draw_deck', 'position': pos, 'keep': True}\n",
    "\n",
    "        # If no good action found, take discard if available, otherwise draw\n",
    "        if not best_action:\n",
    "            if discard_top:\n",
    "                best_action = {'type': 'take_discard', 'position': random.choice(positions)}\n",
    "            else:\n",
    "                best_action = {'type': 'draw_deck', 'position': random.choice(positions), 'keep': True}\n",
    "\n",
    "        return best_action\n",
    "\n",
    "    def calculate_score(self, grid):\n",
    "        \"\"\"Calculate score for a grid (some cards might be None)\"\"\"\n",
    "        scores = [card.score() if card else 0 for card in grid]\n",
    "        total_score = sum(scores)\n",
    "\n",
    "        ranks = [card.rank if card else None for card in grid]\n",
    "        pairs = []\n",
    "        used_positions = set()\n",
    "\n",
    "        for pos1, pos2 in itertools.combinations(range(4), 2):\n",
    "            if (ranks[pos1] and ranks[pos2] and\n",
    "                ranks[pos1] == ranks[pos2] and\n",
    "                pos1 not in used_positions and pos2 not in used_positions):\n",
    "                pairs.append((pos1, pos2))\n",
    "                used_positions.add(pos1)\n",
    "                used_positions.add(pos2)\n",
    "                total_score -= (scores[pos1] + scores[pos2])\n",
    "\n",
    "        return total_score\n",
    "\n",
    "    def evaluate_take_discard_action(self, position, discard_card, player, deck_probs, baseline_expected):\n",
    "        \"\"\"Evaluate taking discard card and placing it at position\"\"\"\n",
    "        new_grid = player.grid.copy()\n",
    "        new_grid[position] = discard_card\n",
    "        new_known = player.known.copy()\n",
    "        new_known[position] = True\n",
    "\n",
    "        known_score = self.calculate_score([card if new_known[i] else None for i, card in enumerate(new_grid)])\n",
    "\n",
    "        # Add expected score for unknown positions\n",
    "        unknown_expected = 0\n",
    "        for i in range(4):\n",
    "            if not new_known[i]:\n",
    "                unknown_expected += player.expected_score_for_unknown_position(deck_probs)\n",
    "\n",
    "        total_expected = known_score + unknown_expected\n",
    "        return baseline_expected - total_expected\n",
    "\n",
    "    def evaluate_draw_deck_action(self, position, player, deck_probs, baseline_expected):\n",
    "        \"\"\"Evaluate drawing from deck and expected outcome at position\"\"\"\n",
    "        total_expected_score = 0\n",
    "\n",
    "        for rank, prob in deck_probs.items():\n",
    "            if prob == 0:\n",
    "                continue\n",
    "\n",
    "            drawn_card = Card(rank, '♠')\n",
    "            new_grid = player.grid.copy()\n",
    "            new_grid[position] = drawn_card\n",
    "            new_known = player.known.copy()\n",
    "            new_known[position] = True\n",
    "\n",
    "            known_score = self.calculate_score([card if new_known[i] else None for i, card in enumerate(new_grid)])\n",
    "\n",
    "            unknown_expected = 0\n",
    "            for i in range(4):\n",
    "                if not new_known[i]:\n",
    "                    unknown_expected += player.expected_score_for_unknown_position(deck_probs)\n",
    "\n",
    "            total_score = known_score + unknown_expected\n",
    "            total_expected_score += prob * total_score\n",
    "\n",
    "        return baseline_expected - total_expected_score\n",
    "\n",
    "class QLearningAgent:\n",
    "    \"\"\"Q-learning agent that actually learns from experience\"\"\"\n",
    "    def __init__(self, learning_rate=0.1, discount_factor=0.9, epsilon=0.2):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = defaultdict(lambda: defaultdict(float))\n",
    "        self.training_mode = True\n",
    "\n",
    "    def get_state_key(self, player, game_state):\n",
    "        \"\"\"Convert game state to a simplified string key for Q-table\"\"\"\n",
    "        # Simplified state representation focusing on key features\n",
    "        # Only track known cards and their scores, not specific suits\n",
    "        known_cards = []\n",
    "        for i, card in enumerate(player.grid):\n",
    "            if player.known[i] and card:\n",
    "                known_cards.append(card.rank)  # Only rank, not suit\n",
    "            else:\n",
    "                known_cards.append('?')\n",
    "\n",
    "        # Sort known cards for consistency (same state regardless of position)\n",
    "        known_cards_sorted = sorted([c for c in known_cards if c != '?'])\n",
    "        unknown_count = known_cards.count('?')\n",
    "\n",
    "        # Include discard top and round for context\n",
    "        discard_top = game_state.discard_pile[-1].rank if game_state.discard_pile else 'None'\n",
    "\n",
    "        return f\"{known_cards_sorted}_{unknown_count}_{discard_top}_{game_state.round}\"\n",
    "\n",
    "    def get_action_key(self, action):\n",
    "        \"\"\"Convert action to a string key\"\"\"\n",
    "        return f\"{action['type']}_{action['position']}\"\n",
    "\n",
    "    def choose_action(self, player, game_state, trajectory=None):\n",
    "        # Get legal actions first\n",
    "        positions = [i for i, known in enumerate(player.known) if not known]\n",
    "        if not positions:\n",
    "            return None\n",
    "\n",
    "        actions = []\n",
    "        if game_state.discard_pile:\n",
    "            for pos in positions:\n",
    "                actions.append({'type': 'take_discard', 'position': pos})\n",
    "        if game_state.deck:\n",
    "            for pos in positions:\n",
    "                actions.append({'type': 'draw_deck', 'position': pos, 'keep': True})\n",
    "\n",
    "        if not actions:\n",
    "            return None\n",
    "\n",
    "        # Epsilon-greedy policy\n",
    "        if self.training_mode and random.random() < self.epsilon:\n",
    "            action = random.choice(actions)\n",
    "        else:\n",
    "            # Choose action with highest Q-value\n",
    "            state_key = self.get_state_key(player, game_state)\n",
    "            best_action = None\n",
    "            best_value = float('-inf')\n",
    "\n",
    "            for action in actions:\n",
    "                action_key = self.get_action_key(action)\n",
    "                q_value = self.q_table[state_key][action_key]\n",
    "                if q_value > best_value:\n",
    "                    best_value = q_value\n",
    "                    best_action = action\n",
    "\n",
    "            action = best_action or random.choice(actions)\n",
    "\n",
    "        # Record action in trajectory for training\n",
    "        if trajectory is not None:\n",
    "            state_key = self.get_state_key(player, game_state)\n",
    "            action_key = self.get_action_key(action)\n",
    "            trajectory.append({\n",
    "                'state_key': state_key,\n",
    "                'action_key': action_key,\n",
    "                'action': action\n",
    "            })\n",
    "\n",
    "        return action\n",
    "\n",
    "    def update(self, state_key, action_key, reward, next_state_key, next_actions):\n",
    "        \"\"\"Update Q-values using Q-learning update rule\"\"\"\n",
    "        max_next_q = 0\n",
    "        if next_actions:\n",
    "            max_next_q = max(self.q_table[next_state_key][self.get_action_key(a)]\n",
    "                           for a in next_actions)\n",
    "\n",
    "        current_q = self.q_table[state_key][action_key]\n",
    "        new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_next_q - current_q)\n",
    "        self.q_table[state_key][action_key] = new_q\n",
    "\n",
    "    def train_on_trajectory(self, trajectory, final_reward, final_score):\n",
    "        \"\"\"Train the agent on a complete game trajectory with improved rewards\"\"\"\n",
    "        if not trajectory:\n",
    "            return\n",
    "\n",
    "        # Update Q-values for each step in the trajectory\n",
    "        for i, step in enumerate(trajectory):\n",
    "            state_key = step['state_key']\n",
    "            action_key = step['action_key']\n",
    "\n",
    "            # Calculate immediate reward for this action\n",
    "            # Give small positive reward for taking actions (encourages exploration)\n",
    "            # The main learning comes from the final reward\n",
    "            immediate_reward = 0.1  # Small positive reward for taking action\n",
    "\n",
    "            # Get next state and actions (if not the last step)\n",
    "            if i < len(trajectory) - 1:\n",
    "                next_step = trajectory[i + 1]\n",
    "                next_state_key = next_step['state_key']\n",
    "                next_actions = [next_step['action']]\n",
    "            else:\n",
    "                next_state_key = state_key  # Terminal state\n",
    "                next_actions = []\n",
    "                # Add final reward to the last action\n",
    "                immediate_reward += final_reward\n",
    "\n",
    "            self.update(state_key, action_key, immediate_reward, next_state_key, next_actions)\n",
    "\n",
    "    def set_training_mode(self, training):\n",
    "        \"\"\"Enable or disable training mode\"\"\"\n",
    "        self.training_mode = training\n",
    "\n",
    "    def get_q_table_size(self):\n",
    "        \"\"\"Get the size of the Q-table for debugging\"\"\"\n",
    "        total_entries = sum(len(actions) for actions in self.q_table.values())\n",
    "        return len(self.q_table), total_entries\n",
    "\n",
    "    def decay_epsilon(self, factor=0.995):\n",
    "        \"\"\"Decay epsilon for better exploration/exploitation balance\"\"\"\n",
    "        self.epsilon = max(0.01, self.epsilon * factor)\n",
    "\n",
    "class GolfGame:\n",
    "    RANKS = ['A', '2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K']\n",
    "    SUITS = ['♠', '♥', '♦', '♣']\n",
    "\n",
    "    def __init__(self, num_players=4, agent_types=None, q_agents=None):\n",
    "        self.num_players = num_players\n",
    "        if agent_types is None:\n",
    "            agent_types = [\"random\"] * num_players\n",
    "        self.players = [Player(f'P{i+1}', agent_types[i]) for i in range(num_players)]\n",
    "        self.agents = self.create_agents(agent_types, q_agents)\n",
    "        self.deck = self.create_deck()\n",
    "        self.discard_pile = []\n",
    "        self.turn = 0  # Player index\n",
    "        self.round = 1\n",
    "        self.max_rounds = 4\n",
    "        self.deal()\n",
    "\n",
    "    def create_agents(self, agent_types, q_agents=None):\n",
    "        agents = []\n",
    "        for i, agent_type in enumerate(agent_types):\n",
    "            if agent_type == \"random\":\n",
    "                agents.append(RandomAgent())\n",
    "            elif agent_type == \"heuristic\":\n",
    "                agents.append(HeuristicAgent())\n",
    "            elif agent_type == \"qlearning\":\n",
    "                # Use persistent Q-learning agent if provided\n",
    "                if q_agents and i < len(q_agents):\n",
    "                    agents.append(q_agents[i])\n",
    "                else:\n",
    "                    agents.append(QLearningAgent())\n",
    "            else:\n",
    "                agents.append(RandomAgent())  # Default to random\n",
    "        return agents\n",
    "\n",
    "    def create_deck(self):\n",
    "        return [Card(rank, suit) for rank, suit in itertools.product(self.RANKS, self.SUITS)]\n",
    "\n",
    "    def deal(self):\n",
    "        random.shuffle(self.deck)\n",
    "        for player in self.players:\n",
    "            for i in range(4):\n",
    "                player.grid[i] = self.deck.pop()\n",
    "        # Start discard pile\n",
    "        self.discard_pile.append(self.deck.pop())\n",
    "\n",
    "    def play_turn(self, player, trajectory=None):\n",
    "        agent = self.agents[self.turn]\n",
    "        action = agent.choose_action(player, self, trajectory)\n",
    "\n",
    "        if not action:\n",
    "            return  # No moves left\n",
    "\n",
    "        if action['type'] == 'take_discard' and self.discard_pile:\n",
    "            # Take from discard pile, swap with pos\n",
    "            new_card = self.discard_pile.pop()\n",
    "            old_card = player.grid[action['position']]\n",
    "            player.grid[action['position']] = new_card\n",
    "            player.known[action['position']] = True\n",
    "            player.add_to_discard_memory(old_card)\n",
    "            self.discard_pile.append(old_card)\n",
    "        elif action['type'] == 'draw_deck' and self.deck:\n",
    "            # Draw from deck\n",
    "            new_card = self.deck.pop()\n",
    "            if action.get('keep', True):\n",
    "                old_card = player.grid[action['position']]\n",
    "                player.grid[action['position']] = new_card\n",
    "                player.known[action['position']] = True\n",
    "                player.add_to_discard_memory(old_card)\n",
    "                self.discard_pile.append(old_card)\n",
    "            else:\n",
    "                player.add_to_discard_memory(new_card)\n",
    "                self.discard_pile.append(new_card)\n",
    "\n",
    "    def all_players_done(self):\n",
    "        return all(all(p.known) for p in self.players)\n",
    "\n",
    "    def next_player(self):\n",
    "        self.turn = (self.turn + 1) % self.num_players\n",
    "        if self.turn == 0:\n",
    "            self.round += 1\n",
    "\n",
    "    def play_game(self, verbose=True, trajectories=None):\n",
    "        if trajectories is None:\n",
    "            trajectories = [None] * self.num_players\n",
    "\n",
    "        # Each player must take exactly 4 turns, so game should last exactly 4 rounds\n",
    "        while self.round <= self.max_rounds:\n",
    "            player = self.players[self.turn]\n",
    "            if verbose:\n",
    "                print(f\"\\n-- {player.name}'s turn (Round {self.round}) --\")\n",
    "                print(f\"Agent: {player.agent_type}\")\n",
    "                print(player)\n",
    "                print(f\"Top of discard: {self.discard_pile[-1]}\")\n",
    "\n",
    "            # Check if player has any moves available\n",
    "            available_positions = [i for i, known in enumerate(player.known) if not known]\n",
    "            if available_positions:\n",
    "                self.play_turn(player, trajectories[self.turn])\n",
    "            else:\n",
    "                # Player has no moves (all cards face-up), but still counts as a turn\n",
    "                if verbose:\n",
    "                    print(f\"{player.name} has no moves available (all cards face-up)\")\n",
    "\n",
    "            self.next_player()\n",
    "\n",
    "        # Reveal all cards\n",
    "        for p in self.players:\n",
    "            p.reveal_all()\n",
    "        if verbose:\n",
    "            print(\"\\n=== FINAL GRIDS ===\")\n",
    "            for p in self.players:\n",
    "                print(f\"{p.name} ({p.agent_type}):\\n{p}\\n\")\n",
    "        scores = [self.calculate_score(p.grid) for p in self.players]\n",
    "        if verbose:\n",
    "            for i, s in enumerate(scores):\n",
    "                print(f\"{self.players[i].name} ({self.players[i].agent_type}) score: {s}\")\n",
    "            winner_idx = scores.index(min(scores))\n",
    "            print(f\"Winner: {self.players[winner_idx].name} ({self.players[winner_idx].agent_type})\")\n",
    "        return scores\n",
    "\n",
    "    def calculate_score(self, grid):\n",
    "        scores = [card.score() if card else 0 for card in grid]\n",
    "        total_score = sum(scores)\n",
    "        ranks = [card.rank if card else None for card in grid]\n",
    "        pairs = []\n",
    "        used = set()\n",
    "        for pos1, pos2 in itertools.combinations(range(4), 2):\n",
    "            if (ranks[pos1] and ranks[pos2] and ranks[pos1] == ranks[pos2]\n",
    "                and pos1 not in used and pos2 not in used):\n",
    "                pairs.append((pos1, pos2))\n",
    "                used.add(pos1)\n",
    "                used.add(pos2)\n",
    "                total_score -= (scores[pos1] + scores[pos2])\n",
    "        return total_score\n",
    "\n",
    "def run_simulations_with_training(num_games=1000, agent_types=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Run multiple simulations with Q-learning training\n",
    "\n",
    "    Args:\n",
    "        num_games: Number of games to simulate\n",
    "        agent_types: List of agent types for each player\n",
    "        verbose: Whether to print detailed output for each game\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with simulation results and statistics\n",
    "    \"\"\"\n",
    "    if agent_types is None:\n",
    "        agent_types = [\"random\", \"heuristic\", \"qlearning\", \"random\"]\n",
    "\n",
    "    num_players = len(agent_types)\n",
    "\n",
    "    # Create persistent Q-learning agents\n",
    "    q_agents = []\n",
    "    for i, agent_type in enumerate(agent_types):\n",
    "        if agent_type == \"qlearning\":\n",
    "            q_agents.append(QLearningAgent(epsilon=0.2))  # Higher epsilon for more exploration\n",
    "        else:\n",
    "            q_agents.append(None)\n",
    "\n",
    "    # Statistics tracking\n",
    "    stats = {\n",
    "        'total_games': num_games,\n",
    "        'agent_types': agent_types,\n",
    "        'wins_by_agent': defaultdict(int),\n",
    "        'scores_by_agent': defaultdict(list),\n",
    "        'average_scores': {},\n",
    "        'win_rates': {},\n",
    "        'score_distributions': defaultdict(list),\n",
    "        'game_durations': [],\n",
    "        'q_learning_progress': [],\n",
    "        'learning_curves': defaultdict(list),  # Track scores over time\n",
    "        'score_by_interval': defaultdict(list)  # Track average scores by intervals\n",
    "    }\n",
    "\n",
    "    print(f\"Running {num_games} simulations with agents: {agent_types}\")\n",
    "    print(\"Q-learning agents will learn from experience!\")\n",
    "\n",
    "    # Track scores for learning curves\n",
    "    interval_size = max(1, num_games // 20)  # 20 intervals for tracking\n",
    "\n",
    "    for game_num in range(num_games):\n",
    "        if verbose and game_num % 100 == 0:\n",
    "            print(f\"Game {game_num + 1}/{num_games}\")\n",
    "\n",
    "        # Create trajectories for Q-learning agents\n",
    "        trajectories = []\n",
    "        for i in range(num_players):\n",
    "            if agent_types[i] == \"qlearning\":\n",
    "                trajectories.append([])\n",
    "            else:\n",
    "                trajectories.append(None)\n",
    "\n",
    "        # Create and play game\n",
    "        game = GolfGame(num_players=num_players, agent_types=agent_types, q_agents=q_agents)\n",
    "        scores = game.play_game(verbose=False, trajectories=trajectories)\n",
    "\n",
    "        # Train Q-learning agents\n",
    "        winner_idx = scores.index(min(scores))\n",
    "        for i, agent_type in enumerate(agent_types):\n",
    "            if agent_type == \"qlearning\" and trajectories[i]:\n",
    "                # Calculate reward: stronger signals for learning\n",
    "                if i == winner_idx:\n",
    "                    reward = 10.0  # Big reward for winning\n",
    "                else:\n",
    "                    # Stronger negative reward based on score\n",
    "                    # Lower scores should have higher rewards\n",
    "                    if scores[i] <= 5:\n",
    "                        reward = 2.0  # Good score\n",
    "                    elif scores[i] <= 10:\n",
    "                        reward = 0.0  # Average score\n",
    "                    elif scores[i] <= 15:\n",
    "                        reward = -2.0  # Bad score\n",
    "                    else:\n",
    "                        reward = -5.0  # Very bad score\n",
    "\n",
    "                q_agents[i].train_on_trajectory(trajectories[i], reward, scores[i])\n",
    "\n",
    "                # Decay epsilon for better learning\n",
    "                if game_num % 100 == 0:  # Decay every 100 games\n",
    "                    q_agents[i].decay_epsilon()\n",
    "\n",
    "        # Record results\n",
    "        winner_agent = agent_types[winner_idx]\n",
    "        stats['wins_by_agent'][winner_agent] += 1\n",
    "\n",
    "        # Record scores for each agent\n",
    "        for i, score in enumerate(scores):\n",
    "            agent_type = agent_types[i]\n",
    "            stats['scores_by_agent'][agent_type].append(score)\n",
    "            stats['score_distributions'][agent_type].append(score)\n",
    "\n",
    "            # Track learning curves (every game)\n",
    "            stats['learning_curves'][agent_type].append(score)\n",
    "\n",
    "        # Record game duration (number of rounds)\n",
    "        stats['game_durations'].append(game.round)\n",
    "\n",
    "        # Track Q-learning progress and scores by intervals\n",
    "        if game_num % 100 == 0 or game_num == num_games - 1:\n",
    "            q_progress = {}\n",
    "            for i, agent_type in enumerate(agent_types):\n",
    "                if agent_type == \"qlearning\":\n",
    "                    states, entries = q_agents[i].get_q_table_size()\n",
    "                    q_progress[f\"qlearning_{i}\"] = {\"states\": states, \"entries\": entries}\n",
    "            stats['q_learning_progress'].append((game_num, q_progress))\n",
    "\n",
    "        # Track average scores by intervals\n",
    "        if (game_num + 1) % interval_size == 0 or game_num == num_games - 1:\n",
    "            interval_start = max(0, game_num - interval_size + 1)\n",
    "            interval_end = game_num + 1\n",
    "\n",
    "            for i, agent_type in enumerate(agent_types):\n",
    "                if agent_type in stats['scores_by_agent']:\n",
    "                    interval_scores = stats['scores_by_agent'][agent_type][interval_start:interval_end]\n",
    "                    avg_score = np.mean(interval_scores)\n",
    "                    stats['score_by_interval'][agent_type].append({\n",
    "                        'interval': len(stats['score_by_interval'][agent_type]) + 1,\n",
    "                        'games': f\"{interval_start+1}-{interval_end}\",\n",
    "                        'avg_score': avg_score,\n",
    "                        'min_score': min(interval_scores),\n",
    "                        'max_score': max(interval_scores)\n",
    "                    })\n",
    "\n",
    "    # Calculate final statistics\n",
    "    for agent_type in agent_types:\n",
    "        if agent_type in stats['scores_by_agent']:\n",
    "            scores = stats['scores_by_agent'][agent_type]\n",
    "            stats['average_scores'][agent_type] = np.mean(scores)\n",
    "            stats['win_rates'][agent_type] = stats['wins_by_agent'][agent_type] / num_games\n",
    "\n",
    "    return stats\n",
    "\n",
    "def run_simulations(num_games=1000, agent_types=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Run multiple simulations and collect statistics (without Q-learning training)\n",
    "\n",
    "    Args:\n",
    "        num_games: Number of games to simulate\n",
    "        agent_types: List of agent types for each player\n",
    "        verbose: Whether to print detailed output for each game\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with simulation results and statistics\n",
    "    \"\"\"\n",
    "    if agent_types is None:\n",
    "        agent_types = [\"random\", \"heuristic\", \"qlearning\", \"random\"]\n",
    "\n",
    "    num_players = len(agent_types)\n",
    "\n",
    "    # Statistics tracking\n",
    "    stats = {\n",
    "        'total_games': num_games,\n",
    "        'agent_types': agent_types,\n",
    "        'wins_by_agent': defaultdict(int),\n",
    "        'scores_by_agent': defaultdict(list),\n",
    "        'average_scores': {},\n",
    "        'win_rates': {},\n",
    "        'score_distributions': defaultdict(list),\n",
    "        'game_durations': []\n",
    "    }\n",
    "\n",
    "    print(f\"Running {num_games} simulations with agents: {agent_types}\")\n",
    "\n",
    "    for game_num in range(num_games):\n",
    "        if verbose and game_num % 100 == 0:\n",
    "            print(f\"Game {game_num + 1}/{num_games}\")\n",
    "\n",
    "        # Create and play game\n",
    "        game = GolfGame(num_players=num_players, agent_types=agent_types)\n",
    "        scores = game.play_game(verbose=False)\n",
    "\n",
    "        # Record results\n",
    "        winner_idx = scores.index(min(scores))\n",
    "        winner_agent = agent_types[winner_idx]\n",
    "        stats['wins_by_agent'][winner_agent] += 1\n",
    "\n",
    "        # Record scores for each agent\n",
    "        for i, score in enumerate(scores):\n",
    "            agent_type = agent_types[i]\n",
    "            stats['scores_by_agent'][agent_type].append(score)\n",
    "            stats['score_distributions'][agent_type].append(score)\n",
    "\n",
    "        # Record game duration (number of rounds)\n",
    "        stats['game_durations'].append(game.round)\n",
    "\n",
    "    # Calculate final statistics\n",
    "    for agent_type in agent_types:\n",
    "        if agent_type in stats['scores_by_agent']:\n",
    "            scores = stats['scores_by_agent'][agent_type]\n",
    "            stats['average_scores'][agent_type] = np.mean(scores)\n",
    "            stats['win_rates'][agent_type] = stats['wins_by_agent'][agent_type] / num_games\n",
    "\n",
    "    return stats\n",
    "\n",
    "def print_simulation_results(stats):\n",
    "    \"\"\"Print formatted simulation results\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SIMULATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total games: {stats['total_games']}\")\n",
    "    print(f\"Agents: {stats['agent_types']}\")\n",
    "\n",
    "    print(\"\\nWIN RATES:\")\n",
    "    for agent_type in stats['agent_types']:\n",
    "        win_rate = stats['win_rates'].get(agent_type, 0)\n",
    "        wins = stats['wins_by_agent'].get(agent_type, 0)\n",
    "        print(f\"  {agent_type}: {win_rate:.2%} ({wins} wins)\")\n",
    "\n",
    "    print(\"\\nAVERAGE SCORES:\")\n",
    "    for agent_type in stats['agent_types']:\n",
    "        avg_score = stats['average_scores'].get(agent_type, 0)\n",
    "        scores = stats['scores_by_agent'].get(agent_type, [])\n",
    "        if scores:\n",
    "            min_score = min(scores)\n",
    "            max_score = max(scores)\n",
    "            print(f\"  {agent_type}: {avg_score:.2f} (range: {min_score}-{max_score})\")\n",
    "\n",
    "    print(f\"\\nAverage game duration: {np.mean(stats['game_durations']):.1f} rounds\")\n",
    "\n",
    "    # Show Q-learning progress if available\n",
    "    if 'q_learning_progress' in stats and stats['q_learning_progress']:\n",
    "        print(\"\\nQ-LEARNING PROGRESS:\")\n",
    "        for game_num, progress in stats['q_learning_progress']:\n",
    "            print(f\"  Game {game_num}: {progress}\")\n",
    "\n",
    "    # Show learning curves and score intervals\n",
    "    if 'score_by_interval' in stats:\n",
    "        print(\"\\nLEARNING CURVES (Score by Intervals):\")\n",
    "        for agent_type in stats['agent_types']:\n",
    "            if agent_type in stats['score_by_interval'] and stats['score_by_interval'][agent_type]:\n",
    "                print(f\"\\n  {agent_type.upper()} LEARNING PROGRESS:\")\n",
    "                intervals = stats['score_by_interval'][agent_type]\n",
    "\n",
    "                # Show first few, middle, and last intervals\n",
    "                to_show = []\n",
    "                if len(intervals) <= 6:\n",
    "                    to_show = intervals\n",
    "                else:\n",
    "                    to_show = intervals[:3] + intervals[len(intervals)//2-1:len(intervals)//2+1] + intervals[-3:]\n",
    "\n",
    "                for interval in to_show:\n",
    "                    print(f\"    Interval {interval['interval']} (Games {interval['games']}): \"\n",
    "                          f\"Avg={interval['avg_score']:.2f}, Range={interval['min_score']}-{interval['max_score']}\")\n",
    "\n",
    "                # Show overall improvement\n",
    "                if len(intervals) >= 2:\n",
    "                    first_avg = intervals[0]['avg_score']\n",
    "                    last_avg = intervals[-1]['avg_score']\n",
    "                    improvement = first_avg - last_avg\n",
    "                    print(f\"    Overall improvement: {improvement:+.2f} points \"\n",
    "                          f\"({first_avg:.2f} → {last_avg:.2f})\")\n",
    "\n",
    "    # Show some interesting statistics\n",
    "    print(\"\\nDETAILED ANALYSIS:\")\n",
    "    for agent_type in stats['agent_types']:\n",
    "        scores = stats['scores_by_agent'].get(agent_type, [])\n",
    "        if scores:\n",
    "            perfect_games = sum(1 for s in scores if s == 0)\n",
    "            print(f\"  {agent_type}: {perfect_games} perfect games (score = 0)\")\n",
    "\n",
    "            # Score distribution\n",
    "            score_counts = defaultdict(int)\n",
    "            for score in scores:\n",
    "                score_counts[score] += 1\n",
    "            most_common_score = max(score_counts.items(), key=lambda x: x[1])\n",
    "            print(f\"    Most common score: {most_common_score[0]} (occurred {most_common_score[1]} times)\")\n",
    "\n",
    "            # For Q-learning agents, show learning trend\n",
    "            if agent_type == \"qlearning\" and 'learning_curves' in stats:\n",
    "                learning_curve = stats['learning_curves'][agent_type]\n",
    "                if len(learning_curve) >= 100:\n",
    "                    first_100_avg = np.mean(learning_curve[:100])\n",
    "                    last_100_avg = np.mean(learning_curve[-100:])\n",
    "                    trend = first_100_avg - last_100_avg\n",
    "                    print(f\"    Learning trend: {trend:+.2f} points improvement \"\n",
    "                          f\"({first_100_avg:.2f} → {last_100_avg:.2f})\")\n",
    "\n",
    "def plot_learning_curves(stats):\n",
    "    \"\"\"Plot learning curves for visualization (if matplotlib is available)\"\"\"\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        for agent_type in stats['agent_types']:\n",
    "            if agent_type in stats['learning_curves']:\n",
    "                scores = stats['learning_curves'][agent_type]\n",
    "                games = list(range(1, len(scores) + 1))\n",
    "\n",
    "                # Plot individual scores with low alpha\n",
    "                plt.scatter(games, scores, alpha=0.1, s=1, label=f'{agent_type} (individual)')\n",
    "\n",
    "                # Plot moving average\n",
    "                window_size = max(1, len(scores) // 50)  # 50 points for moving average\n",
    "                if len(scores) >= window_size:\n",
    "                    moving_avg = []\n",
    "                    for i in range(len(scores)):\n",
    "                        start = max(0, i - window_size // 2)\n",
    "                        end = min(len(scores), i + window_size // 2 + 1)\n",
    "                        moving_avg.append(np.mean(scores[start:end]))\n",
    "                    plt.plot(games, moving_avg, linewidth=2, label=f'{agent_type} (moving avg)')\n",
    "\n",
    "        plt.xlabel('Game Number')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Learning Curves - Score vs Game Number')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.ylim(bottom=0)  # Scores can't be negative\n",
    "\n",
    "        # Save plot\n",
    "        plt.savefig('golf_learning_curves.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"\\nLearning curves plot saved as 'golf_learning_curves.png'\")\n",
    "        plt.show()\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"\\nMatplotlib not available. Install with 'pip install matplotlib' to see learning curves plot.\")\n",
    "\n",
    "def main():\n",
    "    print(\"=== GOLF GAME SIMULATION SUITE WITH Q-LEARNING ===\")\n",
    "\n",
    "    # Example 1: Single game with different agents\n",
    "    print(\"\\n1. Single game example:\")\n",
    "    agent_types = [\"heuristic\", \"random\", \"qlearning\", \"random\"]\n",
    "    game = GolfGame(num_players=4, agent_types=agent_types)\n",
    "    game.play_game(verbose=True)\n",
    "\n",
    "    # Example 2: Run simulations with Q-learning training\n",
    "    print(\"\\n2. Running simulations with Q-learning training...\")\n",
    "    stats = run_simulations_with_training(num_games=1000, agent_types=agent_types, verbose=True)\n",
    "    print_simulation_results(stats)\n",
    "\n",
    "    # Example 3: Plot learning curves\n",
    "    print(\"\\n3. Plotting learning curves...\")\n",
    "    plot_learning_curves(stats)\n",
    "\n",
    "    # Example 4: Compare trained vs untrained Q-learning\n",
    "    print(\"\\n4. Comparing trained vs untrained Q-learning:\")\n",
    "\n",
    "    # Untrained Q-learning\n",
    "    print(\"\\nUntrained Q-learning vs Random:\")\n",
    "    stats_untrained = run_simulations(num_games=20000, agent_types=[\"qlearning\", \"random\"], verbose=False)\n",
    "    print_simulation_results(stats_untrained)\n",
    "\n",
    "    # Trained Q-learning\n",
    "    print(\"\\nTrained Q-learning vs Random:\")\n",
    "    stats_trained = run_simulations_with_training(num_games=20000, agent_types=[\"qlearning\", \"random\"], verbose=False)\n",
    "    print_simulation_results(stats_trained)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
